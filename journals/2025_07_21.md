- **[10:20:24]** Not until I have found out that I can do this (see below) that now I can synthesize Youtube playlist of selected topics of my chosen category, even multiple hundreds of them, although NotebookLM allow up to 300 sources only per notebook. I was thinking about how to manage YT videos and summarize them while I was second year in law school. That was year 2023.
	- ![CleanShot 2025-07-21 at 10.19.50@2x.png](../assets/CleanShot_2025-07-21_at_10.19.50@2x_1753064428155_0.png){:width 664.1705322265625}
	- **NOTE:** Imported multiple youtube links is through "Website" where you can input one link per line.
	- ![CleanShot 2025-07-21 at 10.25.09@2x.png](../assets/CleanShot_2025-07-21_at_10.25.09@2x_1753064764023_0.png)
- The final output:
	- ![CleanShot 2025-07-21 at 10.26.44@2x.png](../assets/CleanShot_2025-07-21_at_10.26.44@2x_1753064879787_0.png)
- The ability of NotebookLM to cite multiple sources:
	- ![CleanShot 2025-07-21 at 10.42.08@2x.png](../assets/CleanShot_2025-07-21_at_10.42.08@2x_1753065845144_0.png){:width 666.9111328125}
- Finally, how to extract all those youtube links? You can do it manually, BUT to automated and extract all youtube links in one (1) playlist, it's easy with python:
	- ```python
	  import os
	  import argparse
	  from googleapiclient.discovery import build
	  from googleapiclient.errors import HttpError
	  
	  def get_playlist_videos(api_key, playlist_id):
	      try:
	          youtube = build('youtube', 'v3', developerKey=api_key)
	          videos = []
	          next_page_token = None
	  
	          while True:
	              request = youtube.playlistItems().list(
	                  part="snippet",
	                  playlistId=playlist_id,
	                  maxResults=50,
	                  pageToken=next_page_token
	              )
	              response = request.execute()
	              
	              for item in response['items']:
	                  video_id = item['snippet']['resourceId']['videoId']
	                  videos.append(f"https://www.youtube.com/watch?v={video_id}")
	              
	              next_page_token = response.get('nextPageToken')
	              if not next_page_token:
	                  break
	  
	          return videos
	      except HttpError as e:
	          print(f"\n‚ùå YouTube API Error: {e}")
	          print("Possible reasons:")
	          print("- Invalid API key")
	          print("- Incorrect playlist ID")
	          print("- API quota exceeded")
	          exit(1)
	      except Exception as e:
	          print(f"\n‚ö†Ô∏è Unexpected error: {e}")
	          exit(1)
	  
	  def save_to_file(videos, filename):
	      with open(filename, 'w') as f:
	          for video in videos:
	              f.write(video + '\n')
	  
	  if __name__ == "__main__":
	      parser = argparse.ArgumentParser(description='Extract YouTube playlist links')
	      parser.add_argument('--api-key', required=True, help='YouTube Data API Key')
	      parser.add_argument('--playlist', required=True, help='Playlist ID or URL')
	      parser.add_argument('--output', default='playlist_links.txt', help='Output filename')
	      args = parser.parse_args()
	  
	      # Extract playlist ID from URL if needed
	      playlist_id = args.playlist
	      if "youtube.com" in playlist_id or "youtu.be" in playlist_id:
	          if "list=" in playlist_id:
	              playlist_id = playlist_id.split("list=")[-1].split("&")[0]
	          elif "/playlist/" in playlist_id:
	              playlist_id = playlist_id.split("/playlist/")[-1].split("/")[0]
	      
	      print(f"üîç Extracting videos from playlist: {playlist_id}")
	      try:
	          video_links = get_playlist_videos(args.api_key, playlist_id)
	          save_to_file(video_links, args.output)
	          print(f"‚úÖ Success! Saved {len(video_links)} links to {args.output}")
	      except Exception as e:
	          print(f"‚ùå Extraction failed: {e}")
	  
	  ```
	- Then on terminal or command prompt:
	- ```text
	  python playlist_extractor.py --api-key API_KEY --playlist PLAYLIST_ID --output youtube_playlist_[NAME].txt
	  ```
	- You need YT api. And that part, you need you do your research HOW.
	- -The Paksiteer **[10:53:17]**