- **[12:18:30]** Important Tips from Bar Topnotchers about **Daily Rest and Sleep**:
	- Atty. Menchie Ona stresses "get enough sleep everyday! (at least 9 hours for me) Your brain processes info while sleeping".
	- Atty. John Paul Lim aimed for 8-12 hours of "pure studying" daily and usually went home by 9-11pm, freeing up weekends for minimal to no studying initially.
	- Atty. Eric David Tan prioritized "at least 6 hours of sleep".
	- Atty. Joan Mae To aimed for "at least 8 hours of sleep!".
	- Atty. Timothy Joseph Lumauig "never studied into the wee hours of the morning" to avoid burnout and prioritized sleep over cramming the night before exams.
	- Atty. Johana T. Sunga emphasizes getting "8 hours of sleep" and studying during "peak hours".
	- Atty. Raoul Angelo D. Atadero notes, "When you need to, nap. There‚Äôs no point studying when you‚Äôre sleepy" and "don‚Äôt force yourself to study when you can‚Äôt process or retain what you‚Äôre reading anyway".
	- Atty. Luz Danielle O. Bolong made "8 hours sleep... non-negotiable".
	- Atty. Irene Marie P. Qua, while cutting back on certain activities, did not eliminate them and stressed, "Rest. But, earn it".
	- Atty. Mae Diane Azores recommends "at least six to eight hours of sleep for your brain to work properly" and advises waking up early and going to bed early to take advantage of peak focus hours and train for exam days.
	- Atty. Bernice Pinol Rodriguez likewise states the importance of "get enough sleep night after night" because "if your brain is not going to be at its best so it's not worth it".
	- Czar Dayday, for his time, notes there was no social media or Netflix, reducing temptation for distraction. The 2023 topnotcher quoted by Bilyonaryo News Channel explicitly advises, "don't deprive yourself of breaks" to prevent burnout.
- **[10:20:24]** Not until I have found out that I can do this (see below) that now I can synthesize Youtube playlist of selected topics of my chosen category, even multiple hundreds of them, although NotebookLM allow up to 300 sources only per notebook. I was thinking about how to manage YT videos and summarize them while I was second year in law school. That was year 2023.
	- ![CleanShot 2025-07-21 at 10.19.50@2x.png](../assets/CleanShot_2025-07-21_at_10.19.50@2x_1753064428155_0.png){:width 664.1705322265625}
	- **NOTE:** Imported multiple youtube links is through "Website" where you can input one link per line.
	- ![CleanShot 2025-07-21 at 10.25.09@2x.png](../assets/CleanShot_2025-07-21_at_10.25.09@2x_1753064764023_0.png)
- The final output:
	- ![CleanShot 2025-07-21 at 10.26.44@2x.png](../assets/CleanShot_2025-07-21_at_10.26.44@2x_1753064879787_0.png)
- The ability of NotebookLM to cite multiple sources:
	- ![CleanShot 2025-07-21 at 10.42.08@2x.png](../assets/CleanShot_2025-07-21_at_10.42.08@2x_1753065845144_0.png){:width 666.9111328125}
- Finally, how to extract all those youtube links? You can do it manually, BUT to automated and extract all youtube links in one (1) playlist, it's easy with python:
	- ```python
	  import os
	  import argparse
	  from googleapiclient.discovery import build
	  from googleapiclient.errors import HttpError
	  
	  def get_playlist_videos(api_key, playlist_id):
	      try:
	          youtube = build('youtube', 'v3', developerKey=api_key)
	          videos = []
	          next_page_token = None
	  
	          while True:
	              request = youtube.playlistItems().list(
	                  part="snippet",
	                  playlistId=playlist_id,
	                  maxResults=50,
	                  pageToken=next_page_token
	              )
	              response = request.execute()
	              
	              for item in response['items']:
	                  video_id = item['snippet']['resourceId']['videoId']
	                  videos.append(f"https://www.youtube.com/watch?v={video_id}")
	              
	              next_page_token = response.get('nextPageToken')
	              if not next_page_token:
	                  break
	  
	          return videos
	      except HttpError as e:
	          print(f"\n‚ùå YouTube API Error: {e}")
	          print("Possible reasons:")
	          print("- Invalid API key")
	          print("- Incorrect playlist ID")
	          print("- API quota exceeded")
	          exit(1)
	      except Exception as e:
	          print(f"\n‚ö†Ô∏è Unexpected error: {e}")
	          exit(1)
	  
	  def save_to_file(videos, filename):
	      with open(filename, 'w') as f:
	          for video in videos:
	              f.write(video + '\n')
	  
	  if __name__ == "__main__":
	      parser = argparse.ArgumentParser(description='Extract YouTube playlist links')
	      parser.add_argument('--api-key', required=True, help='YouTube Data API Key')
	      parser.add_argument('--playlist', required=True, help='Playlist ID or URL')
	      parser.add_argument('--output', default='playlist_links.txt', help='Output filename')
	      args = parser.parse_args()
	  
	      # Extract playlist ID from URL if needed
	      playlist_id = args.playlist
	      if "youtube.com" in playlist_id or "youtu.be" in playlist_id:
	          if "list=" in playlist_id:
	              playlist_id = playlist_id.split("list=")[-1].split("&")[0]
	          elif "/playlist/" in playlist_id:
	              playlist_id = playlist_id.split("/playlist/")[-1].split("/")[0]
	      
	      print(f"üîç Extracting videos from playlist: {playlist_id}")
	      try:
	          video_links = get_playlist_videos(args.api_key, playlist_id)
	          save_to_file(video_links, args.output)
	          print(f"‚úÖ Success! Saved {len(video_links)} links to {args.output}")
	      except Exception as e:
	          print(f"‚ùå Extraction failed: {e}")
	  
	  ```
	- Then on terminal or command prompt:
	- ```text
	  python playlist_extractor.py --api-key API_KEY --playlist PLAYLIST_ID --output youtube_playlist_[NAME].txt
	  ```
	- You need YT api. And that part, you need you do your research HOW.
	- -The Paksiteer **[10:53:17]**